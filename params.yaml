# ======================================
# STAGE 04: FEATURE TRANSFORMATION
# ======================================
feature_transformation:
  test_size: 0.2
  random_state: 42

# ======================================
# STAGE 05: MULTI-MODEL TRAINING
# ======================================
models:
  # 1. Random Forest
  RandomForest:
    n_estimators: 140
    max_depth: 10
    class_weight: "balanced"
    random_state: 42

  # 2. Gradient Boosting
  # Note: Scikit-learn's GBT doesn't have class_weight; 
  # We use a lower learning rate and deeper trees to capture minority patterns
  GradientBoosting:
    n_estimators: 150
    learning_rate: 0.05
    max_depth: 5
    random_state: 42

  # 3. AdaBoost
  AdaBoost:
    n_estimators: 100
    learning_rate: 0.5
    random_state: 42

  # 4. CatBoost: Increasing the scale weight to force higher Recall
  CatBoost:
    iterations: 150
    learning_rate: 0.1
    depth: 6
    scale_pos_weight: 3.0  # Gives 3x more importance to Class 0
    verbose: False
    random_state: 42

  # 5. MLP
  MLP:
    hidden_layer_sizes: [100, 50]
    solver: "adam"
    learning_rate_init: 0.001
    max_iter: 20  
    random_state: 42